{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 no. of peoples\n",
      "Found face 1 at top:225 , right:339 , bottom:354 , left:210\n",
      "3\n",
      "Found face 2 at top:247 , right:1369 , bottom:355 , left:1262\n",
      "3\n",
      "Found face 3 at top:254 , right:597 , bottom:383 , left:468\n",
      "3\n",
      "Found face 4 at top:223 , right:915 , bottom:331 , left:808\n",
      "3\n",
      "Found face 5 at top:211 , right:1130 , bottom:319 , left:1023\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "face_exp_model = model_from_json(open(\"E:/Courses/Face Recognition/facial_expression_model_structure.json\",\n",
    "                                      \"r\").read())\n",
    "face_exp_model.load_weights(\"E:/Courses/Face Recognition/facial_expression_model_weights.h5\")\n",
    "emotion_label = ('angry','disgust','fear','happy','sad','surprise','neutral')\n",
    "\n",
    "image1 = cv2.imread('E:/Courses/Face Recognition/group image2.jpg')\n",
    "number_of_faces = face_recognition.face_locations(image1, model='hog')\n",
    "print(\"There are {} no. of peoples\".format(len(number_of_faces)))\n",
    "\n",
    "for index, current_face_location in enumerate(number_of_faces):\n",
    "    top_pos, right_pos, bottom_pos, left_pos = current_face_location\n",
    "    print(\"Found face {} at top:{} , right:{} , bottom:{} , left:{}\".format(index+1, top_pos, right_pos,\n",
    "                                                                        bottom_pos, left_pos))\n",
    "    cv2.rectangle(image1, (left_pos,top_pos), (right_pos,bottom_pos), (0,0,255), 2)\n",
    "    \n",
    "    current_face_image = image1[top_pos:bottom_pos, left_pos:right_pos]\n",
    "    current_face_image = cv2.cvtColor(current_face_image, cv2.COLOR_BGR2GRAY)\n",
    "    current_face_image = cv2.resize(current_face_image, (48,48))\n",
    "    \n",
    "    input_image = image.img_to_array(current_face_image)\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "    input_image = input_image/255\n",
    "    expression = face_exp_model.predict(input_image)\n",
    "    max_index = np.argmax(expression[0])\n",
    "    print(max_index)\n",
    "    emotion_label1 = emotion_label[max_index]\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    cv2.putText(image1, emotion_label1, (left_pos,bottom_pos), font, 0.5, (255,255,255), 1)\n",
    "cv2.imshow(\"Image\", image1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
